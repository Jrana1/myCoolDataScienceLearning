{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dfd33c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;text-align:center\">Gradient Descent and Cost Function</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0ced4",
   "metadata": {},
   "source": [
    "<img src='s1.png' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581f0c9",
   "metadata": {},
   "source": [
    "# how to find the line which fit the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cb9e7",
   "metadata": {},
   "source": [
    "<img src='s2.png' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906eac2",
   "metadata": {},
   "source": [
    "$\\Delta$ is called erro. The result of the equation is called **Mean Squared Error** or **MSE**. **MSE** is also known as **Cost Function**. \n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^n\\left(y_i - y_{predicted}\\right)^2$$\n",
    "Here $y_i$ is the actual point on graph, $n$ is the number of points.\n",
    "we can also replace $y_{predicted}$ with $(mx_i+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99913a79",
   "metadata": {},
   "source": [
    "**Gradient descent** is an algorithm which finds best fit line for given training data set in a shortest possible iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e353d13",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;flex-direction:row\">\n",
    "    <img src='s3.png' width=350 height=400> \n",
    "    <img src='s4.png' width=350 height=400> \n",
    "    <img src='s5.png' width=350 height=400>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8bf108",
   "metadata": {},
   "source": [
    "<h1>How to calculate <span style=\"color:green\">Gradient Descent</span> in python.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33c72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27956d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 0.00715, b 0.0016, iteration 0, cost 8.0\n",
      "m 0.014266427500000001, b 0.0031950325, iteration 1, cost 7.9751625\n",
      "m 0.021349438483750004, b 0.0047851203156250004, iteration 2, cost 7.950439078125\n",
      "m 0.028399188210443786, b 0.0063702861565474375, iteration 3, cost 7.925829204612187\n",
      "m 0.03541583121808451, b 0.00795055262697934, iteration 4, cost 7.9013323521595105\n",
      "m 0.04239952132683469, b 0.009525942226162189, iteration 5, cost 7.8769479959142465\n",
      "m 0.049350411642350586, b 0.011096477348854513, iteration 6, cost 7.852675613461625\n",
      "m 0.056268654559101135, b 0.012662180285817214, iteration 7, cost 7.828514684813506\n",
      "m 0.06315440176367144, b 0.014223073224296634, iteration 8, cost 7.8044646923971035\n",
      "m 0.07000780423805095, b 0.015779178248505388, iteration 9, cost 7.780525121043771\n",
      "m 0.07682901226290628, b 0.017330517340100954, iteration 10, cost 7.756695457977829\n",
      "m 0.08361817542083899, b 0.018877112378662044, iteration 11, cost 7.732975192805454\n",
      "m 0.09037544259962804, b 0.02041898514216277, iteration 12, cost 7.709363817503611\n",
      "m 0.09710096199545733, b 0.021956157307444577, iteration 13, cost 7.685860826409046\n",
      "m 0.10379488111612815, b 0.02348865045068604, iteration 14, cost 7.662465716207318\n",
      "m 0.11045734678425682, b 0.02501648604787042, iteration 15, cost 7.639177985921897\n",
      "m 0.11708850514045734, b 0.02653968547525108, iteration 16, cost 7.615997136903294\n",
      "m 0.12368850164650934, b 0.028058270009814732, iteration 17, cost 7.592922672818263\n",
      "m 0.13025748108851135, b 0.029572260829742538, iteration 18, cost 7.56995409963903\n",
      "m 0.13679558758001928, b 0.03108167901486906, iteration 19, cost 7.547090925632596\n",
      "m 0.14330296456517053, b 0.03258654554713907, iteration 20, cost 7.524332661350069\n",
      "m 0.14977975482179337, b 0.034086881311062286, iteration 21, cost 7.501678819616057\n",
      "m 0.15622610046450203, b 0.03558270709416591, iteration 22, cost 7.479128915518109\n",
      "m 0.16264214294777735, b 0.03707404358744515, iteration 23, cost 7.456682466396203\n",
      "m 0.1690280230690331, b 0.03856091138581161, iteration 24, cost 7.434338991832279\n",
      "m 0.17538388097166824, b 0.04004333098853957, iteration 25, cost 7.412098013639831\n",
      "m 0.1817098561481046, b 0.04152132279971028, iteration 26, cost 7.389959055853539\n",
      "m 0.1880060874428109, b 0.042994907128654065, iteration 27, cost 7.367921644718949\n",
      "m 0.19427271305531246, b 0.044464104190390506, iteration 28, cost 7.345985308682211\n",
      "m 0.20050987054318703, b 0.04592893410606647, iteration 29, cost 7.324149578379844\n",
      "m 0.2067176968250466, b 0.04738941690339219, iteration 30, cost 7.302413986628576\n",
      "m 0.21289632818350543, b 0.048845572517075234, iteration 31, cost 7.280778068415207\n",
      "m 0.2190459002681344, b 0.05029742078925254, iteration 32, cost 7.2592413608865325\n",
      "m 0.22516654809840136, b 0.051744981469920404, iteration 33, cost 7.237803403339311\n",
      "m 0.2312584060665982, b 0.05318827421736246, iteration 34, cost 7.216463737210275\n",
      "m 0.23732160794075388, b 0.054627318598575704, iteration 35, cost 7.195221906066193\n",
      "m 0.24335628686753438, b 0.0560621340896945, iteration 36, cost 7.174077455593974\n",
      "m 0.2493625753751288, b 0.05749274007641267, iteration 37, cost 7.153029933590819\n",
      "m 0.2553406053761223, b 0.05891915585440355, iteration 38, cost 7.132078889954419\n",
      "m 0.2612905081703556, b 0.06034140062973819, iteration 39, cost 7.1112238766732\n",
      "m 0.26721241444777116, b 0.06175949351930151, iteration 40, cost 7.090464447816606\n",
      "m 0.27310645429124625, b 0.0631734535512066, iteration 41, cost 7.0698001595254425\n",
      "m 0.2789727571794128, b 0.06458329966520705, iteration 42, cost 7.049230570002243\n",
      "m 0.28481145198946406, b 0.0659890507131074, iteration 43, cost 7.028755239501701\n",
      "m 0.2906226669999485, b 0.06739072545917163, iteration 44, cost 7.008373730321134\n",
      "m 0.29640652989355026, b 0.06878834258052983, iteration 45, cost 6.988085606790996\n",
      "m 0.30216316775985724, b 0.07018192066758291, iteration 46, cost 6.967890435265432\n",
      "m 0.30789270709811595, b 0.07157147822440549, iteration 47, cost 6.947787784112881\n",
      "m 0.31359527381997365, b 0.07295703366914683, iteration 48, cost 6.927777223706717\n",
      "m 0.3192709932522078, b 0.07433860533443001, iteration 49, cost 6.907858326415939\n",
      "m 0.3249199901394429, b 0.07571621146774919, iteration 50, cost 6.888030666595895\n",
      "m 0.33054238864685437, b 0.077089870231865, iteration 51, cost 6.868293820579062\n",
      "m 0.33613831236286046, b 0.07845959970519818, iteration 52, cost 6.848647366665858\n",
      "m 0.34170788430180105, b 0.07982541788222128, iteration 53, cost 6.829090885115505\n",
      "m 0.3472512269066044, b 0.08118734267384867, iteration 54, cost 6.809623958136925\n",
      "m 0.35276846205144136, b 0.0825453919078246, iteration 55, cost 6.790246169879687\n",
      "m 0.3582597110443672, b 0.0838995833291096, iteration 56, cost 6.770957106424991\n",
      "m 0.3637250946299514, b 0.08524993460026493, iteration 57, cost 6.751756355776697\n",
      "m 0.36916473299189495, b 0.08659646330183542, iteration 58, cost 6.732643507852393\n",
      "m 0.37457874575563566, b 0.08793918693273031, iteration 59, cost 6.713618154474506\n",
      "m 0.3799672519909412, b 0.08927812291060261, iteration 60, cost 6.694679889361454\n",
      "m 0.38533037021449057, b 0.09061328857222638, iteration 61, cost 6.675828308118838\n",
      "m 0.3906682183924427, b 0.09194470117387252, iteration 62, cost 6.657063008230679\n",
      "m 0.39598091394299406, b 0.09327237789168266, iteration 63, cost 6.638383589050688\n",
      "m 0.40126857373892383, b 0.09459633582204138, iteration 64, cost 6.619789651793587\n",
      "m 0.4065313141101274, b 0.09591659198194667, iteration 65, cost 6.601280799526457\n",
      "m 0.411769250846138, b 0.0972331633093787, iteration 66, cost 6.582856637160139\n",
      "m 0.416982499198637, b 0.09854606666366683, iteration 67, cost 6.5645167714406725\n",
      "m 0.4221711738839518, b 0.09985531882585498, iteration 68, cost 6.546260810940763\n",
      "m 0.427335389085543, b 0.10116093649906524, iteration 69, cost 6.528088366051302\n",
      "m 0.43247525845647944, b 0.10246293630885983, iteration 70, cost 6.5099990489729205\n",
      "m 0.4375908951219017, b 0.10376133480360135, iteration 71, cost 6.491992473707582\n",
      "m 0.4426824116814747, b 0.1050561484548114, iteration 72, cost 6.474068256050218\n",
      "m 0.44774992021182836, b 0.10634739365752748, iteration 73, cost 6.456226013580396\n",
      "m 0.45279353226898716, b 0.10763508673065827, iteration 74, cost 6.4384653656540305\n",
      "m 0.45781335889078834, b 0.1089192439173373, iteration 75, cost 6.420785933395133\n",
      "m 0.462809510599289, b 0.11019988138527483, iteration 76, cost 6.403187339687601\n",
      "m 0.4677820974031618, b 0.11147701522710823, iteration 77, cost 6.385669209167036\n",
      "m 0.4727312288000798, b 0.11275066146075076, iteration 78, cost 6.368231168212616\n",
      "m 0.4776570137790899, b 0.11402083602973856, iteration 79, cost 6.350872844938991\n",
      "m 0.48255956082297574, b 0.1152875548035762, iteration 80, cost 6.333593869188219\n",
      "m 0.48743897791060886, b 0.11655083357808056, iteration 81, cost 6.316393872521753\n",
      "m 0.49229537251928984, b 0.11781068807572305, iteration 82, cost 6.2992724882124405\n",
      "m 0.4971288516270779, b 0.11906713394597036, iteration 83, cost 6.282229351236586\n",
      "m 0.5019395217151098, b 0.12032018676562357, iteration 84, cost 6.265264098266026\n",
      "m 0.5067274887699084, b 0.12156986203915562, iteration 85, cost 6.248376367660269\n",
      "m 0.5114928582856798, b 0.12281617519904735, iteration 86, cost 6.231565799458641\n",
      "m 0.5162357352666006, b 0.12405914160612184, iteration 87, cost 6.214832035372493\n",
      "m 0.5209562242290936, b 0.12529877654987734, iteration 88, cost 6.198174718777426\n",
      "m 0.5256544292040938, b 0.12653509524881845, iteration 89, cost 6.181593494705568\n",
      "m 0.5303304537393034, b 0.12776811285078601, iteration 90, cost 6.165088009837877\n",
      "m 0.5349844009014366, b 0.1289978444332853, iteration 91, cost 6.148657912496478\n",
      "m 0.5396163732784535, b 0.1302243050038127, iteration 92, cost 6.1323028526370456\n",
      "m 0.5442264729817841, b 0.13144750950018094, iteration 93, cost 6.116022481841213\n",
      "m 0.5488148016485418, b 0.13266747279084273, iteration 94, cost 6.099816453309021\n",
      "m 0.5533814604437269, b 0.133884209675213, iteration 95, cost 6.083684421851397\n",
      "m 0.5579265500624191, b 0.13509773488398955, iteration 96, cost 6.067626043882674\n",
      "m 0.5624501707319605, b 0.13630806307947219, iteration 97, cost 6.0516409774131485\n",
      "m 0.5669524222141283, b 0.1375152088558805, iteration 98, cost 6.035728882041656\n",
      "m 0.5714334038072977, b 0.13871918673967015, iteration 99, cost 6.019889418948202\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(x,y):\n",
    "    m_curr,b_curr=0,0\n",
    "    iterations=100\n",
    "    n=len(x)\n",
    "    learning_rate=0.0001\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr*x+b_curr\n",
    "        cost=(1/n)*sum([val for val in (y-y_predicted)])\n",
    "        md=-(2/n) * sum(x*(y-y_predicted))\n",
    "        bd=-(2/n) * sum(y-y_predicted)\n",
    "        m_curr=m_curr-learning_rate*md\n",
    "        b_curr=b_curr-learning_rate*bd\n",
    "        print(f'm {m_curr}, b {b_curr}, iteration {i}, cost {cost}')\n",
    "x=np.array([1,3,0,9])\n",
    "y=np.array([8,9,3,12])\n",
    "gradient_descent(x,y)\n",
    "## here we have to fine tune and try minimize the cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
